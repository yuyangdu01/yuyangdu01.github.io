---
permalink: /research/
title: ""
author_profile: true
redirect_from: 
  - /md/
  - /markdown.html
---

<div align="center">
  <img src="../images/research01.jpg" width="30%">
  <img src="../images/research02.png" width="30%">
  <img src="../images/research03.png" width="30%">
</div>
<div align="center">
  <table style="border:none; border-collapse: collapse;">
    <tr>
      <td style="border:none; padding: 10px;" width="30%" align="center">Efficient AIs at Edge</td>
      <td style="border:none; padding: 10px;" width="30%" align="center">Highly Reliable Wireless Communications at Edge</td>
      <td style="border:none; padding: 10px;" width="30%" align="center">AI-Wireless Integration for Mobile Edge Applications</td>
    </tr>
  </table>
</div>
<br>


<div style="text-align: left;">
<details>
  <summary>
    <span style="text-align: center; font-size: 24px; font-weight: bold;">Efficient AIs at Edge</span>
  </summary>
  <br>

  <p style="text-align: justify;">A core challenge for intelligent mobile systems lies in the inherent conflict between the growing user demand for powerful AIs and the severe resource constraints at the edge. To address this, my recent works design and implement cost-effective LLM agents for edge scenarios. I use LLM post-training techniques (e.g., supervised fine-tuning, knowledge distillation, and model alignment) to boost the performance of lightweight models for specific edge applications. I am also actively exploring distributed LLM inference powered by MoE architecture. My research enables the aggregation of fragmented and underutilized computational resources across massive distributed IoT devices for supporting large-scale LLMs at the edge.
  </p>
  <span>Selected Publications:</span>
  <br>
  <span style="font-size: 14px; color: grey;">(underlines for student co-advised, * for equal contribution, # for corresponding author/project lead)</span>
  <br><br>
  
  <p style="font-size: 18px; font-weight: bold;">Application-Specific Agents:</p>
  <ul>
    <li>
      <span><u>Yang, X.</u>, <b>Du, Y.</b><span style="color: #e97132;">#</span>, Chen, K#., Liew, S. C.<span style="color: #e97132;">#</span>, …, Heng, P. A.# "IndusGCC: A Data Benchmark and Evaluation Framework for GUI-Based General Computer Control in Industrial Automation." <i>NeurIPS 2025.</i> [<a href="https://openreview.net/pdf?id=Og72OdBv8G">OpenReview</a>]</span>
    </li>
    <li>
      <span><u>Chen, K.</u>, <b>Du, Y.</b><span style="color: #e97132;">#</span>, Li, J., Cao, H., Guo, M., Dang, X., Li, L., Qiu, J., Chen, G. <span style="color: #e97132;">#</span>, Heng, P. A. "ChemMiner: A Large Language Model Agent System for Chemical Data Mining." <i>ICCV 2025.</i> [<a href="https://openaccess.thecvf.com/content/ICCV2025W/VisionDocs/html/Chen_ChemMiner_A_Large_Language_Model_Agent_System_for_Chemical_Literature_ICCVW_2025_paper.html">Proceedings</a>]</span>
    </li>
    <li>
      <span><u>Wang, L.</u><span style="color: #45b0e1;">*</span>, <b>Du, Y.</b><span style="color: #45b0e1;">*</span>, Long, X., Liu, X., Chen, K., Liew, S. C., " Cellular-X: An LLM-empowered Cellular Agent for Efficient Base Station Operations." <i>ACM Mobisys 2025.</i> [<a href="https://dl.acm.org/doi/pdf/10.1145/3711875.3734372">ACM Library</a>]</span>
    </li>
  </ul>

  <p style="font-size: 18px; font-weight: bold;">LLM Post-train:</p>
  <ul>
    <li>
      <span><u>Wang, L.</u><span style="color: #45b0e1;">*</span>, Du, Y.<span style="color: #45b0e1;">*</span>, Lin, J., Chen, K., Liew, S. C. "Rephrase and Contrast: Fine-Tuning Language Models for Enhanced Understanding of Communication and Computer Networks." <i>IEEE ICNC 2025.</i> [<a href="https://ieeexplore.ieee.org/abstract/document/10993716/">IEEE Xplore</a>]</span>
    </li>
    <li>
      <span><u>Zhang, Y.</u>, <u>Chen, X.</u>, <u>Chen, K.</u>, <b>Du, Y.</b><span style="color: #e97132;">#</span>, Dang, X., …, Heng, P. A., "The Dual-use Dilemma in LLMs: Do Empowering Ethical Capacities Make a Degraded Utility?" <i>Major Revision at Nature Communication.</i> [<a href="https://arxiv.org/abs/2501.13952">Arxiv</a>]</span>
    </li>
  </ul>

  <p style="font-size: 18px; font-weight: bold;">Distributed MoE Inference:</p>
  <ul>
    <li>
      <span><u>Wang, L.</u><span style="color: #45b0e1;">*</span>, <b>Du, Y.</b><span style="color: #45b0e1;">*</span><span style="color: #e97132;">#</span>, Pan, Y., Liew, S. C.<span style="color: #e97132;">#</span>, Liu, Y., Chen, K. "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference", <i>Under Review at OSDI’26.</i> [<a href="https://arxiv.org/abs/2512.03927">Arxiv</a>]</span>
    </li>
  </ul>
</details>
<hr>


</div>
